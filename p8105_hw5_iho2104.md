p8105_hw5_iho2104
================
Ixtaccihuatl Obregon
2023-11-15

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.3     ✔ readr     2.1.4
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.0
    ## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(dplyr)
library(purrr)
library(ggplot2)
library(broom)
```

## Problem One

Describe the raw data.

``` r
homicide =
  read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

There are 52179 observations and 12 variables that include: ‘uid’,
‘report_date’, ‘victim_last’\[name\], ‘victim_first’ \[name\],
‘victim_race’, ‘victim_age”, ’victim_sex’, ‘city’, ‘state’, ‘lat’,
‘lon’, and ‘disposition’. Identification, victim names, and locations
are given in the dataframe.

Create a ‘city_state’ variable and summarize within cities to obtain the
total number of homicides and the number of unsolved homicides.

``` r
homicide_df = 
  homicide |> 
  mutate(city_state = paste(city, state, sep = ", "))

disposition = 
  homicide_df |> 
  group_by(city_state) |> 
  summarize(homicide_n = n(), 
            unsolved_n = sum(disposition == "Closed without arrest" | disposition == "Open/No arrest"))

head(disposition)
```

    ## # A tibble: 6 × 3
    ##   city_state      homicide_n unsolved_n
    ##   <chr>                <int>      <int>
    ## 1 Albuquerque, NM        378        146
    ## 2 Atlanta, GA            973        373
    ## 3 Baltimore, MD         2827       1825
    ## 4 Baton Rouge, LA        424        196
    ## 5 Birmingham, AL         800        347
    ## 6 Boston, MA             614        310

For the city of Baltimore, MD, use the ‘prop.test’ function to estimate
the proportion of homicides that are unsolved; save the output of
prop.test as an R object, apply the ‘broom::tidy’ to this object and
pull the estimated proportion and confidence intervals from the
resulting tidy dataframe.

``` r
baltimore_md = 
disposition |> 
  filter(city_state == "Baltimore, MD")

prop_test_result = prop.test(x = baltimore_md$unsolved_n, n = baltimore_md$homicide_n) |> 
  tidy() |> 
  select(estimate, conf.low, conf.high)


baltimore_md
```

    ## # A tibble: 1 × 3
    ##   city_state    homicide_n unsolved_n
    ##   <chr>              <int>      <int>
    ## 1 Baltimore, MD       2827       1825

Now run prop.test for each of the cities in your dataset, and extract
both the proportion of unsolved homicides and the confidence interval
for each. Do this within a “tidy” pipeline, making use of purrr::map,
purrr::map2, list columns and unnest as necessary to create a tidy
dataframe with estimated proportions and CIs for each city.

``` r
result_df = disposition |> 
  group_by(city_state) |> 
  nest() |> 
  mutate(
    prop_test_result = map(data, ~prop.test(x = .x$unsolved_n, n = .x$homicide_n) |> 
    tidy())) |> 
  unnest(prop_test_result) |> 
  select(city_state, estimate, conf.low, conf.high)
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `prop_test_result = map(data, ~tidy(prop.test(x = .x$unsolved_n,
    ##   n = .x$homicide_n)))`.
    ## ℹ In group 49: `city_state = "Tulsa, AL"`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

``` r
result_df
```

    ## # A tibble: 51 × 4
    ## # Groups:   city_state [51]
    ##    city_state      estimate conf.low conf.high
    ##    <chr>              <dbl>    <dbl>     <dbl>
    ##  1 Albuquerque, NM    0.386    0.337     0.438
    ##  2 Atlanta, GA        0.383    0.353     0.415
    ##  3 Baltimore, MD      0.646    0.628     0.663
    ##  4 Baton Rouge, LA    0.462    0.414     0.511
    ##  5 Birmingham, AL     0.434    0.399     0.469
    ##  6 Boston, MA         0.505    0.465     0.545
    ##  7 Buffalo, NY        0.612    0.569     0.654
    ##  8 Charlotte, NC      0.300    0.266     0.336
    ##  9 Chicago, IL        0.736    0.724     0.747
    ## 10 Cincinnati, OH     0.445    0.408     0.483
    ## # ℹ 41 more rows

Create a plot that shows the estimates and CIs for each city – check out
geom_errorbar for a way to add error bars based on the upper and lower
limits. Organize cities according to the proportion of unsolved
homicides.

``` r
result_df = 
  result_df |> 
  arrange(estimate)  

ggplot(result_df, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  labs(
    title = "Proportion of Unsolved Homicides for Each City",
    x = "City",
    y = "Estimated Proportion",
    caption = "Error bars represent confidence intervals"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

![](p8105_hw5_iho2104_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

## Problem Two

``` r
## CON
file_paths_c = list.files("data", pattern = "con_\\d+\\.csv", full.names = TRUE)
controls = map_dfr(file_paths_c, read.csv) |> 
  mutate(id = c("con_01", "con_02", "con_03", "con_04", "con_05", "con_06", "con_07", "con_08", "con_09", "con_10" )) |> 
  select(id, everything()) 

## EXP
file_paths_e = list.files("data", pattern = "exp_\\d+\\.csv", full.names = TRUE)
exp = map_dfr(file_paths_e, read.csv) |> 
  mutate(id = c("exp_01", "exp_02", "exp_03", "exp_04", "exp_05", "exp_06", "exp_07", "exp_08", "exp_09", "exp_10" )) |> 
  select(id, everything()) 

# combined arms 
df = rbind(controls, exp) |> 
  separate(col = id, into = c("arm", "id"), sep = "_") |> 
  pivot_longer(cols = starts_with("week"), names_to = "week", values_to = "value") 

df_plot = df |> 
  mutate(sub_id = paste(arm, id, sep = "_"))

# spaghetti plot 
df_plot |> 
  ggplot(aes(x = week, y = value, group = sub_id, color = arm)) +
  geom_line() +
  labs(title = "Spaghetti Plot", x = "Time", y = "Value")
```

![](p8105_hw5_iho2104_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

The plot shows the experiment arm to have higher values compared to the
control arm. During week 3 through week 8, the experimental arm shows a
noticeable set of higher values compared to the control arm.

## Problem Three

Following chunk attempted to integrate a for loop and function to obtain
3500 observations consisting of variables: mu, iteration, estimate, and
p.value.

``` r
fix_n = 30 
fix_sigma = 5 
mu_val= c(0,1,2,3,4,5,6)
datasets = vector("list", length = 5000)

for (i in 1:5000) {
  dataset = rnorm(fix_n, mean = mu_val, sd = fix_sigma) # Generate a dataset
  t_test_results = lapply(mu_val, function(mu) {      # Function
  t_test_result = t.test(dataset, mu = mu)            # Perform t-test
  tidy_result = tidy(t_test_result)                   # Extract estimates and p-value 
  tibble::tibble(estimate = tidy_result$estimate[1],  # Assuming one-sided t-test
                   p_value = tidy_result$p.value[1])  # Assuming one-sided t-test
})
  
datasets[[i]] = do.call(rbind, t_test_results) 
}

results_df = do.call(rbind, datasets)
results_df = mutate(results_df, estimate = sub("sub_str", "", estimate)) |> 
select(estimate, p_value)
head(results_df)
```

I used the for-loop 7 times to generate my dataframe with 35000 obs and
4 variables.

``` r
#mu_val = 0 
fix_n = 30
fix_sigma = 5
mu = 0
datasets <- vector("list", length = 5000)

for (i in 1:5000) {
  dataset = rnorm(fix_n, mean = mu, sd = fix_sigma)
  t_test_result = t.test(dataset, mu = 0)
  tidy_result = broom::tidy(t_test_result)
  datasets[[i]] = tibble::tibble(
    mu = mu, 
    iteration = i, 
    estimate = tidy_result$estimate[1],
    p_value = tidy_result$p.value[1])
}


results_df_0 = do.call(rbind, datasets)


results_df_0 = mutate(results_df_0, estimate = sub("sub_str", "", estimate))  # Remove sub_str from estimate
results_df_0 = results_df_0 |> 
  select(mu, iteration,estimate, p_value)

head(results_df_0)
```

    ## # A tibble: 6 × 4
    ##      mu iteration estimate           p_value
    ##   <dbl>     <int> <chr>                <dbl>
    ## 1     0         1 -1.48030814506356    0.171
    ## 2     0         2 0.467071308511612    0.515
    ## 3     0         3 -0.990800193624962   0.324
    ## 4     0         4 -0.563945970254727   0.597
    ## 5     0         5 -0.434755448884016   0.634
    ## 6     0         6 -0.755651200897013   0.384

``` r
#mu_val = 1
fix_n = 30
fix_sigma = 5
mu = 1
datasets <- vector("list", length = 5000)

for (i in 1:5000) {
  dataset = rnorm(fix_n, mean = mu, sd = fix_sigma)
  t_test_result = t.test(dataset, mu = 0)
  tidy_result = tidy(t_test_result)
  datasets[[i]] = tibble::tibble(
    mu = mu, 
    iteration = i, 
    estimate = tidy_result$estimate[1],
    p_value = tidy_result$p.value[1])
}

results_df_1a = do.call(rbind, datasets)
results_df_1a = mutate(results_df_1a, estimate = sub("sub_str", "", estimate), 
                      estimate = as.numeric(estimate))
results_df_1a = results_df_1a |> 
  select(mu, iteration,estimate, p_value)

head(results_df_1a)
```

    ## # A tibble: 6 × 4
    ##      mu iteration estimate p_value
    ##   <dbl>     <int>    <dbl>   <dbl>
    ## 1     1         1    0.679  0.393 
    ## 2     1         2    0.553  0.570 
    ## 3     1         3    0.721  0.510 
    ## 4     1         4    1.62   0.0851
    ## 5     1         5    0.398  0.623 
    ## 6     1         6    1.97   0.0496

``` r
#mu_val = 2
fix_n = 30
fix_sigma = 5
mu = 2
datasets <- vector("list", length = 5000)

for (i in 1:5000) {
  dataset = rnorm(fix_n, mean = mu, sd = fix_sigma)
  t_test_result = t.test(dataset, mu = 0)
  tidy_result = broom::tidy(t_test_result)
  datasets[[i]] = tibble::tibble(
    mu = mu, 
    iteration = i, 
    estimate = tidy_result$estimate[1],
    p_value = tidy_result$p.value[1])
}

results_df_2 = do.call(rbind, datasets)
results_df_2 = mutate(results_df_2, estimate = sub("sub_str", "", estimate))
results_df_2 = results_df_2 |> 
  select(mu, iteration,estimate, p_value)

head(results_df_2)
```

    ## # A tibble: 6 × 4
    ##      mu iteration estimate           p_value
    ##   <dbl>     <int> <chr>                <dbl>
    ## 1     2         1 3.38037861702798  0.000326
    ## 2     2         2 1.53981919228742  0.0626  
    ## 3     2         3 0.511429429031388 0.610   
    ## 4     2         4 1.77053841159811  0.0419  
    ## 5     2         5 2.99953626594457  0.000908
    ## 6     2         6 1.25168520571516  0.188

``` r
#mu_val = 3
fix_n = 30
fix_sigma = 5
mu = 3
datasets <- vector("list", length = 5000)

for (i in 1:5000) {
  dataset = rnorm(fix_n, mean = mu, sd = fix_sigma)
  t_test_result = t.test(dataset, mu = 0)
  tidy_result = broom::tidy(t_test_result)
  datasets[[i]] = tibble::tibble(
    mu = mu, 
    iteration = i, 
    estimate = tidy_result$estimate[1],
    p_value = tidy_result$p.value[1])
}

results_df_3 = do.call(rbind, datasets)
results_df_3 = mutate(results_df_3, estimate = sub("sub_str", "", estimate))
results_df_3 = results_df_3 |> 
  select(mu, iteration,estimate, p_value)

head(results_df_3)
```

    ## # A tibble: 6 × 4
    ##      mu iteration estimate           p_value
    ##   <dbl>     <int> <chr>                <dbl>
    ## 1     3         1 2.25744835679218 0.0125   
    ## 2     3         2 3.87660525238177 0.000429 
    ## 3     3         3 3.48305173294608 0.0000754
    ## 4     3         4 2.34395868808234 0.0129   
    ## 5     3         5 4.60654194231987 0.000160 
    ## 6     3         6 4.54126756807171 0.0000518

``` r
#mu_val = 4
fix_n = 30
fix_sigma = 5
mu = 4
datasets <- vector("list", length = 5000)

for (i in 1:5000) {
  dataset = rnorm(fix_n, mean = mu, sd = fix_sigma)
  t_test_result = t.test(dataset, mu = 0)
  tidy_result = broom::tidy(t_test_result)
  datasets[[i]] = tibble::tibble(
    mu = mu, 
    iteration = i, 
    estimate = tidy_result$estimate[1],
    p_value = tidy_result$p.value[1])
}

results_df_4 = do.call(rbind, datasets)
results_df_4 = mutate(results_df_4, estimate = sub("sub_str", "", estimate))
results_df_4 = results_df_4 |> 
  select(mu, iteration,estimate, p_value)

head(results_df_4)
```

    ## # A tibble: 6 × 4
    ##      mu iteration estimate           p_value
    ##   <dbl>     <int> <chr>                <dbl>
    ## 1     4         1 3.96582182886234 0.0000967
    ## 2     4         2 4.31628836458708 0.000631 
    ## 3     4         3 4.94653545634976 0.0000164
    ## 4     4         4 3.45284528232628 0.00198  
    ## 5     4         5 3.59755138994057 0.000427 
    ## 6     4         6 3.80379934369609 0.0000680

``` r
#mu_val = 5
fix_n = 30
fix_sigma = 5
mu = 5
datasets <- vector("list", length = 5000)

for (i in 1:5000) {
  dataset = rnorm(fix_n, mean = mu, sd = fix_sigma)
  t_test_result = t.test(dataset, mu = 0)
  tidy_result = broom::tidy(t_test_result)
  datasets[[i]] = tibble::tibble(
    mu = mu, 
    iteration = i, 
    estimate = tidy_result$estimate[1],
    p_value = tidy_result$p.value[1])
}

results_df_5 = do.call(rbind, datasets)
results_df_5 = mutate(results_df_5, estimate = sub("sub_str", "", estimate))
results_df_5 = results_df_5 |> 
  select(mu, iteration,estimate, p_value)

head(results_df_5)
```

    ## # A tibble: 6 × 4
    ##      mu iteration estimate             p_value
    ##   <dbl>     <int> <chr>                  <dbl>
    ## 1     5         1 4.06560668541156 0.0000690  
    ## 2     5         2 5.12103891141979 0.000000237
    ## 3     5         3 3.5325737883658  0.000273   
    ## 4     5         4 5.99215411705984 0.00000441 
    ## 5     5         5 5.93639238053099 0.000000604
    ## 6     5         6 3.8362296876613  0.000792

``` r
#mu_val = 6
fix_n = 30
fix_sigma = 5
mu = 6
datasets <- vector("list", length = 5000)

for (i in 1:5000) {
  dataset = rnorm(fix_n, mean = mu, sd = fix_sigma)
  t_test_result = t.test(dataset, mu = 0)
  tidy_result = broom::tidy(t_test_result)
  datasets[[i]] = tibble::tibble(
    mu = mu, 
    iteration = i, 
    estimate = tidy_result$estimate[1],
    p_value = tidy_result$p.value[1])
}

results_df_6 = do.call(rbind, datasets)
results_df_6 = mutate(results_df_6, estimate = sub("sub_str", "", estimate))
results_df_1 = results_df_6 |> 
  select(mu, iteration,estimate, p_value)

head(results_df_6)
```

    ## # A tibble: 6 × 4
    ##      mu iteration estimate          p_value
    ##   <dbl>     <int> <chr>               <dbl>
    ## 1     6         1 6.52142829982945 7.50e- 8
    ## 2     6         2 6.5337980458224  1.01e-11
    ## 3     6         3 5.81102411430479 3.27e- 7
    ## 4     6         4 6.08753695592168 3.55e- 7
    ## 5     6         5 4.38904238414681 1.51e- 6
    ## 6     6         6 6.3482609651312  2.31e- 7

``` r
combined_results = rbind(results_df_0, results_df_1a, results_df_2, results_df_3, results_df_4, results_df_5, results_df_6) |> 
  mutate(estimate = as.numeric(estimate))
```

Make a plot showing the proportion of times the null was rejected (the
power of the test) on the y axis and the true value of μ on the x axis.
Describe the association between effect size and power.

``` r
combined_results_power = 
  combined_results |> 
   mutate(reject = case_when(
    p_value > 0.05 ~ FALSE,
    p_value < 0.05 ~ TRUE
  )) |> 
  group_by(mu) |> 
  summarise(count = sum(reject)) |> 
  mutate(proportion = count / 5000)

combined_results_power |> 
  ggplot(aes(x = mu, y = proportion, fill = mu)) +
  geom_col() +
  labs(title = "Proportiobn of Rejected Hypotheses per mu",
       x = "True Value of mu", y = "Proportion Rejected")
```

![](p8105_hw5_iho2104_files/figure-gfm/unnamed-chunk-17-1.png)<!-- -->

As the true mu increase, the proportion rejection increases, showing
effect size impacts power (in this case increases).

Make a plot showing the average estimate of μ^ on the y axis and the
true value of μ on the x axis.

``` r
average_estimates = combined_results |> 
  group_by(mu) |> 
  summarize(avg_estimate = mean(estimate))

average_estimates = average_estimates[order(average_estimates$mu), ]

ggplot(average_estimates, aes(x = mu, y = avg_estimate)) +
  geom_line() +
  labs(title = "Average Estimate of μ hat vs. True Value of μ", x = "True Value of μ", y = "Average Estimate of μ") +
  theme_minimal()
```

![](p8105_hw5_iho2104_files/figure-gfm/unnamed-chunk-18-1.png)<!-- -->

Make a second plot (or overlay on the first) the average estimate of
μ^only in samples for which the null was rejected on the y axis and the
true value of μ on the x axis.

``` r
rejected_estimates = 
  combined_results |> 
  mutate(reject = case_when(
  p_value > 0.05 ~ FALSE,
  p_value < 0.05 ~ TRUE
  )) |> 
  filter(reject == TRUE) |> 
  group_by(mu) |>
  summarize(muhat = mean(estimate))

ggplot(rejected_estimates, aes(x = mu, y = muhat)) +
  geom_line() +
  labs(title = "Rejected Estimates of μ hat vs. True Value of μ", x = "True Value of μ", y = "Rejected of μ") +
  theme_minimal()
```

![](p8105_hw5_iho2104_files/figure-gfm/unnamed-chunk-19-1.png)<!-- -->

Is the sample average of μ^across tests for which the null is rejected
approximately equal to the true value of μ ? Why or why not? When mu is
0, 1, 2, 3, the rejection of mu is higher. When mu is 4, 5, 6, the
rejection of mu follows the average estimates. When the true value is
higher then the mu of the samples is different compared to 0.
